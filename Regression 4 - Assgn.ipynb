{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce33446-cd88-414a-ad1e-9503e378f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1.\n",
    "\n",
    "# Lasso Regression is a linear regression that adds a penalty to the absolute values of the coefficients in the loss function.\n",
    "# It encourges sparsity in the model, effectively performing feature selection.\n",
    "# It automatically selects a subset of the most relevant features.\n",
    "# Lasso can exclude some coefficents entirely, simplifying the model.\n",
    "# It introduces a bias-variance trade-off in the model.\n",
    "# Suitable for high-dimensional data and variable importance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f67784-f03a-4de2-afe9-da4035b0ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "# Lasso Regression automatically performs feture selection.\n",
    "# It can identify and exclude irrelevant or redundant features.\n",
    "# This simplifies the model, making it more interpretable.\n",
    "# Lasso helps avoid overfitting by setting some coefficients to zero.\n",
    "# It balances bias and variance, leading to more robust models.\n",
    "# It can help mitigate the problem of multicollinearity by automatically selecting a subset of relevant features and setting others' coeffients to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17be0902-22a4-4e02-b2ce-fd401043775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "# Lasso Regression encourages sparse models.\n",
    "# It helps in automatic feature selection.\n",
    "# Features with non-zero coefficients are important.\n",
    "# Coefficients indicate feature influence.\n",
    "# Simplicity in model interpretation.\n",
    "# Magnitude reflects feature impact.\n",
    "# Zero coefficients imply feature exclusion.\n",
    "# Sign shows direction of influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44accc3b-90fa-4059-a659-59ae3ad76b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Alpha is the main tuning parameter.\n",
    "# Higher alpha increases regularization strength.\n",
    "# It shrinks coefficients more aggressively.\n",
    "# Feature selection effect is enhanced.\n",
    "# Lower alpha reduces regularization.\n",
    "# It's closer to OLS regression.\n",
    "# Requires cross-validation to find optimal alpha.\n",
    "# Balancing act between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1f09cd-7b0b-41be-9a3f-889d25bc371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "\n",
    "# Lasso Regression is primarily suited for linear regression problems.\n",
    "# It may not effectively handle non-linear regression problems on its own.\n",
    "# For non-linear problems, we might have to consider combining Lasso with techniques like polynomial regression or kernel methods.\n",
    "# Lasso's primary strength is in linear feature selection and preventing overfitting.\n",
    "# For strictly non-linear problems, other regression methods like decision trees or neural networks might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb7e149-f7a2-4967-81fa-1648bbf10146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6.\n",
    "\n",
    "# Ridge uses L2 regularization, which adds the squared magnitude of coefficients to the cost function.\n",
    "# Lasso uses L1 regularization, which adds the absolute magnitude of coefficients to the cost function.\n",
    "# Ridge tends to shrink all coefficients but doesn't lead to exact zeros, which means it rarely performs feature selection.\n",
    "# Lasso tends to drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "# Ridge is generally preferred when all features are believed to be relevant.\n",
    "# Lasso is preferred when there are many features, and some are expected to be irrelevant, as it automatically selects a subset of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77b8462-d704-4d1f-aa1c-0d9d1b1621c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7.\n",
    "\n",
    "# Lasso Regression can help with multicollinearity.\n",
    "# It achieves this by shrinking some coefficients to zero, effectively choosing one feature over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8590df02-c105-4484-91ab-064d45dd1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8.\n",
    "\n",
    "# The optimal value of the alpha in Lasso Regression is chosen using cross-validation.\n",
    "# We can use techniques like k-fold cross-validation to test different values of alpha.\n",
    "# Some libraries and frameworks provide built-in methods for performing cross-validated hyperparameter tuning, simplifying the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284a493-ba87-45d2-ba06-e70a31ecc124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
